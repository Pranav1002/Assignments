{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6798fb6b-d39c-4b9a-a25a-90cf1c7810b7",
   "metadata": {},
   "source": [
    "# Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae903aa3-7e3d-41f5-9a60-56b5e035f767",
   "metadata": {},
   "source": [
    "- Ridge regression is a linear model for regression like ordinary least squares (OLS) regression. However, there is a difference that helps make the Ridge regression more regularized and thus avoid the problem of overfitting. The OLS model seeks to find the coefficients that minimize the mean squared error. On the other hand, Ridge regression adds a penalty term to the loss function that shrinks the coefficients towards zero. This penalty term is called L2 regularization.\n",
    "- The L2 regularization term is added to the sum of squared residuals in the loss function. The amount of regularization is controlled by a hyperparameter λ (lambda). When λ is zero, Ridge regression is equivalent to OLS regression. When λ is very large, all coefficients are close to zero and the model becomes a null model.\n",
    "- In summary, Ridge regression differs from OLS regression in that it adds an L2 regularization term to the loss function that shrinks the coefficients towards zero. This helps prevent overfitting and makes Ridge regression more regularized than OLS regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ff3a5-6c07-4748-8ec8-b0ca9d8c140c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d20a0-ecde-410a-b223-2873ece1eb4b",
   "metadata": {},
   "source": [
    "# Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a6a52-23f9-46da-892a-670619404dc9",
   "metadata": {},
   "source": [
    "- Ridge Regression assumes that there is a linear relationship between the independent variables and the dependent variable. It also assumes that the variance of the errors is constant across all levels of the independent variables (homoscedasticity). Additionally, Ridge Regression assumes that the errors are normally distributed and independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a2ac6-e024-47d1-a6bf-818a2c2b509c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d428fb-682a-4bd8-aca0-a5fbe9e1a659",
   "metadata": {},
   "source": [
    "# Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7fb32-9c6b-4955-80b3-a37777271da7",
   "metadata": {},
   "source": [
    "- The value of the tuning parameter (lambda) in Ridge Regression can be selected by cross-validation. Ridge Regression involves tuning a hyperparameter, lambda. The model is run many times for different values of lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30884676-4ffb-4f43-8e12-6760e78b3ef9",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509bc3d6-77a1-4b68-80d3-c168f522e9ea",
   "metadata": {},
   "source": [
    "# Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6658b5-0715-43e1-ba1c-d7749f6a3db9",
   "metadata": {},
   "source": [
    "- Yes, Ridge Regression can be used for feature selection by shrinking the coefficients of irrelevant or redundant features towards zero. However, Lasso regression is typically a better choice for feature selection because it can set coefficients to exactly zero, effectively removing irrelevant features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d4acc-5c58-48ad-a575-c4255943c30e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2971e16-3995-4919-82c0-fa8652e9132c",
   "metadata": {},
   "source": [
    "# Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0191c-3bc1-4f83-b44c-ed7786495700",
   "metadata": {},
   "source": [
    "- Ridge regression decreases the parameters of low contributing variables towards zero, but not exactly to zero, and stabilizes the parameter variance of the least squares estimator in the presence of multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04548315-318b-47a8-b063-aae5c087f936",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b4b77-5021-4d67-a93a-a64ca1357a1d",
   "metadata": {},
   "source": [
    "# Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f5b79-fc73-4809-9549-d2c108085595",
   "metadata": {},
   "source": [
    "- Yes, Ridge Regression can handle both categorical and continuous independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758a517-363c-4bb6-9045-c8802596805a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b381d-725f-4afb-9272-35195360fd70",
   "metadata": {},
   "source": [
    "# Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53563978-6633-4125-9a9d-0556d86fd17a",
   "metadata": {},
   "source": [
    "-  Ridge Regression gives biased estimates that depend on the value of alpha. This means that you cannot directly interpret the coefficients as the effect of each predictor on the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542e7b8-6a56-451a-a68a-d169e932b812",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f2884-4302-4d27-a5d5-c0a1622e1978",
   "metadata": {},
   "source": [
    "# Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a28de-cf0d-44f2-90fb-33d3125dc3c5",
   "metadata": {},
   "source": [
    "- Yes, Ridge Regression can be used for time-series data analysis. \n",
    "- Ridge regression models can be used on datasets that have many correlated features. Usually correlated features are a big problem for regression models, but when you introduce the L2 penalty into a regression model, the negative impact of correlated features is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e2a2f-3744-4be2-b959-9aec4e107fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
