{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228e6787-4312-4c6d-9775-e3b823617de4",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25328ad-361b-4966-b62a-2eaa50f044f1",
   "metadata": {},
   "source": [
    "- Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables: One variable, denoted x, is regarded as the predictor, explanatory, or independent variable.The other variable, denoted y, is regarded as the response, outcome, or dependent variable.\n",
    "- An example of simple linear regression is when we predict rent based on square feet alone.\n",
    "\n",
    "- Multiple linear regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable. The variables we are using to predict the value of the dependent variable are called independent variables.\n",
    "- An example of multiple linear regression is when we predict the price of a house based on its size and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f8f38-ff79-48c6-97d2-487dfbd7b17e",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6f84b-8bcc-4afb-b83b-88db1d9efa2f",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e09f5-ca8e-4f45-b999-cf4fa0f6ed3e",
   "metadata": {},
   "source": [
    "- The assumptions of linear regression are:\n",
    "\n",
    "1) Linearity between independent and dependent variables.\n",
    "2) Homoscedasticity of residuals or equal variances.\n",
    "3) No multicollinearity in the data.\n",
    "4) No autocorrelation in residuals.\n",
    "5) Number of observations greater than the number of predictors.\n",
    "6) Each observation is unique.\n",
    "7) Predictors are distributed normally.\n",
    "\n",
    "- Methods of checking the assumption.\n",
    "\n",
    "1) Linearity\n",
    "2) Homoscedasticity\n",
    "3) Multicollinearity\n",
    "4) Autocorrelation\n",
    "5) Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965862c7-b76c-44c6-ab34-f666217eeec3",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2a120-a452-47f3-ba8b-71bafc39cebe",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fba3d-816b-4f69-878c-05d92957c10b",
   "metadata": {},
   "source": [
    "- The slope and intercept in a linear regression model are used to define the linear relationship between two variables and can be used to estimate an average rate of change. \n",
    "- The slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis. The greater the magnitude of the slope, the steeper the line and the greater the rate of change. \n",
    "- The slope is calculated as the change in y divided by the change in x.\n",
    "-  The intercept is calculated as the point where the line crosses the y-axis when x is equal to zero.\n",
    "\n",
    "- Ex: Suppose you are interested in predicting the price of a house based on its size. You collect data on 10 houses and find that on average, a 1000 square foot house sells for Rs. 2000000 and a 2000 square foot house sells for Rs. 4000000. You can use linear regression to model this relationship. The slope of the regression line would be Rs. 2000 per square foot, indicating that for each additional square foot of living space, the price of the house increases by Rs. 200. The intercept would be Rs. 0, indicating that a house with 0 square foot would have a price of $0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9db46a-c89d-45ed-adb7-506cd2beceff",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8344e-bddb-4305-acec-441cce2771b0",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd48066-b3dd-451d-8539-b7d8bbc3cefc",
   "metadata": {},
   "source": [
    "- Gradient descent is an optimization algorithm used to minimize the cost function of a machine learning model by iteratively adjusting the model’s parameters. The cost function is a measure of how well the model fits the training data. The goal of gradient descent is to find the values of the model’s parameters that minimize the cost function.\n",
    "\n",
    "- The algorithm works by calculating the gradient of the cost function with respect to each parameter. The gradient is a vector that points in the direction of steepest ascent. By taking steps in the opposite direction of the gradient, we can iteratively adjust the parameters to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473476a5-0d82-4c85-a2bd-4db8c330402a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb40b3c-1570-4c9b-a0f3-8a5325df5879",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24082492-fb43-4b5a-9592-25b9ba4a8779",
   "metadata": {},
   "source": [
    "- Multiple linear regression is a statistical technique that uses several independent variables to predict the outcome of a dependent variable. It is an extension of simple linear regression, which uses only one independent variable to predict the outcome of a dependent variable.\n",
    "- In multiple linear regression, the relationship between the dependent variable and each independent variable is modeled using a linear equation. The coefficients of these equations are estimated using a method called least squares. The goal of multiple linear regression is to find the coefficients that minimize the sum of the squared differences between the predicted values and the actual values.\n",
    "- The main difference between simple linear regression and multiple linear regression is that in simple linear regression, there is only one independent variable, while in multiple linear regression, there are two or more independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b952eb1-b935-4d9a-97db-2810eb2de9d8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a7896-f5fd-4c47-863e-e7dd1049c420",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af661ac-ec35-4be0-926c-c999802bf36d",
   "metadata": {},
   "source": [
    "- Multicollinearity is a statistical phenomenon in which two or more predictor variables in a multiple regression model are highly correlated. This can cause problems because it can be difficult to determine the effect of each variable on the dependent variable when they are so closely related.\n",
    "- One way to detect multicollinearity is to look at the correlation matrix of the independent variables. If there are high correlations between some of the variables, then there may be multicollinearity.\n",
    "- Another way is to look at the variance inflation factor (VIF) for each variable. The VIF measures how much the variance of the estimated coefficient is increased due to multicollinearity.\n",
    "- There are several ways to address multicollinearity. One way is to remove one or more of the correlated variables from the model. Another way is to combine the correlated variables into a single variable using principal component analysis (PCA) or factor analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bf94e-8b84-4c43-9c73-fef311b3468f",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636828c-0e8a-49ad-8d91-4196520a7e29",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bbf45-423a-40a1-8c3f-183a07915acf",
   "metadata": {},
   "source": [
    "- Polynomial regression is a form of linear regression in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. In other words, it is a linear regression model with polynomial terms.\n",
    "- The main difference between linear and polynomial regression is that linear regression models the relationship between two variables as a straight line, while polynomial regression models the relationship between two variables as a curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca03a73-3e55-4248-8b37-40870223a077",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc8a4e5-bbc5-48c1-8d1e-31459c7d9579",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d48a68-5196-4e15-873b-706c88e698a3",
   "metadata": {},
   "source": [
    "---> Advantages of using polynomial regression:\n",
    "- Polynomial regression provides a better approximation of the relationship between the dependent and independent variables than linear regression2.\n",
    "- A broad range of functions can be fit under polynomial regression2.\n",
    "- Polynomial regression basically fits a wide range of curvature\n",
    "\n",
    "---> Disadvantages of polynomial regression:\n",
    "- The presence of one or two outliers in the data can seriously affect the results of the nonlinear analysis3.\n",
    "- Polynomial regression is too sensitive to outliers4.\n",
    "- These are overly reliant on outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1c077-d34f-4926-b3b1-702a91b3de3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
