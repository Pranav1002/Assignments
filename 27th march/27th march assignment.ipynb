{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5ef211-93af-4db7-821b-92479e6439bb",
   "metadata": {},
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04148c57-db1a-439f-9fbb-41d01afd4d85",
   "metadata": {},
   "source": [
    "- R-squared is a goodness-of-fit measure for linear regression models. It indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.\n",
    "- R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale.\n",
    "- R-squared is calculated by dividing the sum of squares of residuals (SSres) by the total sum of squares (SStot). The formula for R-squared is:\n",
    "     R-squared = 1 - (SSres / SStot)\n",
    "- A high R-squared value indicates that your model fits the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e0807-8634-48d1-86fa-ef2fad2496d9",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425db079-6bbe-4be0-a377-d8d5c7f62b43",
   "metadata": {},
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbe755-9a5a-4fea-8d5a-e61c371fcf6f",
   "metadata": {},
   "source": [
    "- Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance alone.\n",
    "- Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]\n",
    "- where n is the sample size and p is the number of independent variables.\n",
    "- The difference between adjusted R-squared and regular R-squared is that adjusted R-squared accounts for the number of predictors in the model. Adjusted R-squared penalizes the model for adding irrelevant predictors, while R-squared may increase with added predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a28c8-82d0-4682-b03e-dc4f0b20d805",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8bc42-cdf8-4fef-b96f-1af5b00b7521",
   "metadata": {},
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e53bd-ea3c-4a92-ae2e-b9f217846484",
   "metadata": {},
   "source": [
    "- Adjusted R-squared is more appropriate than regular R-squared when you have a large number of independent variables in your model. \n",
    "- Adjusted R-squared accounts for the number of predictors in the model and penalizes the model for adding irrelevant predictors. This means that adjusted R-squared will be lower than regular R-squared when you add irrelevant predictors to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec6611-90d9-40a7-9280-8cf9463057e9",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18939bb-923c-41da-bfff-a7c7735df8b5",
   "metadata": {},
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc2ff4-9a31-4101-bed4-3dfed9b22967",
   "metadata": {},
   "source": [
    "- RMSE, MSE, and MAE are metrics used to evaluate the performance of regression models. They are used to measure the difference between the predicted values and the actual values.\n",
    "\n",
    "- Root Mean Squared Error (RMSE) is the square root of the average of the squared differences between predicted and actual values. It is a measure of how far apart the predicted values are from the actual values. \n",
    "- RMSE = sqrt(1/n * sum((y_pred - y_actual)^2))\n",
    "\n",
    "- Mean Squared Error (MSE) is the average of the squared differences between predicted and actual values. It is a measure of how far apart the predicted values are from the actual values.\n",
    "- MSE = 1/n * sum((y_pred - y_actual)^2)\n",
    "\n",
    "- Mean Absolute Error (MAE) is the average of the absolute differences between predicted and actual values. It is a measure of how far apart the predicted values are from the actual values.\n",
    "- MAE = 1/n * sum(abs(y_pred - y_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffee14-d1cb-4b45-aff9-4261335729c2",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100523eb-c9c5-469c-bcc6-c445d3fc940e",
   "metadata": {},
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f42179-2cb1-43aa-b301-c47423a0e66a",
   "metadata": {},
   "source": [
    "- Root Mean Squared Error (RMSE) is more sensitive to outliers than MAE. It is a measure of how far apart the predicted values are from the actual values. \n",
    "---> Advantages:\n",
    "\n",
    "Less extreme losses even for larger values.\n",
    "More sensitive to outliers than MAE.\n",
    "\n",
    "\n",
    "---> Disadvantages:\n",
    "\n",
    "RMSE is still a linear scoring function, so again, near minima, the gradient is sudden.\n",
    "\n",
    "- Mean Squared Error (MSE) is a measure of how far apart the predicted values are from the actual values.\n",
    "---> Advantages:\n",
    "\n",
    "It has nice mathematical properties which makes it easier to work with.\n",
    "It penalizes larger errors more heavily than smaller errors.\n",
    "\n",
    "\n",
    "---> Disadvantages:\n",
    "\n",
    "It is not as interpretable as MAE.\n",
    "It can be heavily influenced by outliers\n",
    "\n",
    "\n",
    "- Mean Absolute Error (MAE) is a measure of how far apart the predicted values are from the actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa77349-1ab9-4436-a17a-e6dacfbafb9d",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab09553-97b4-44cf-9fb2-e6dd02f9bfa1",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcef863-88b0-4c59-b298-d95282ee9c9d",
   "metadata": {},
   "source": [
    "- Lasso and Ridge are both regularization techniques used in linear regression analysis to prevent overfitting. \n",
    "- The main difference between the two is the way they add a penalty term to the cost function.\n",
    "- Lasso adds the absolute value of the magnitude of coefficients as a penalty term while Ridge adds the square of the magnitude of coefficients as a penalty term.\n",
    "- This difference makes Lasso more likely to produce sparse models with fewer features while Ridge tends to produce models with all features.\n",
    "- If you have many features with high correlation and you need to take away the useless features then LASSO is the better solution. \n",
    "-  If the number of features greater than the number of observations and many features with multi-collinearity, Ridge regularization is a better solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87d763-c7cb-4ff0-9cd2-983db7b15cd4",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a637e-f3be-470a-946d-4b7d74def0e6",
   "metadata": {},
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b105f6f-f077-4a8c-b788-f78b6683c313",
   "metadata": {},
   "source": [
    "- Regularized linear models are a type of linear regression model that adds a penalty term to the loss function. This penalty term is used to prevent overfitting by shrinking the coefficients towards zero. The two most common types of regularization are L1 regularization (Lasso) and L2 regularization (Ridge).\n",
    "- L1 regularization adds an absolute value of the coefficients to the loss function while L2 regularization adds a squared value of the coefficients to the loss function.\n",
    "- For example, let’s say you have a dataset with 1000 features and only 100 samples. If you fit a linear regression model to this dataset without regularization, it is likely that the model will overfit. Regularization can help prevent overfitting by adding a penalty term to the loss function that discourages large coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d99e0-2b30-47ae-aea8-7d4204d5c6e7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d5d97-5c59-4567-89d7-cb79ed7287ec",
   "metadata": {},
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbd16d-9ebf-475c-9f49-22de638fad4d",
   "metadata": {},
   "source": [
    "- Regularized linear models have some limitations. One of the main limitations is that they may not always be the best choice for regression analysis. For example, if you have a small dataset with few features, regularization may not be necessary and may even hurt performance.\n",
    "- Another limitation is that regularization can be sensitive to the choice of hyperparameters.\n",
    "- If the hyperparameters are not chosen correctly, regularization can lead to underfitting or overfitting.\n",
    "-  If the hyperparameters are not chosen correctly, regularization can lead to underfitting or overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c78b2-3110-4994-92f2-dea90c8ba980",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73041ed-7704-4f77-b1d3-49df3dc3f628",
   "metadata": {},
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de4362-7d36-421f-bd2d-1f16e974c584",
   "metadata": {},
   "source": [
    "- Based on this comparison, Model B could be considered the better performer since it has a lower average error. However, the choice of metric depends on the specific goals of the analysis and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c174f-e6a6-4687-8b61-81da23bf7e2c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e86659-45b5-4396-a5fc-28464e06d74f",
   "metadata": {},
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5916b2-cac0-4e10-a43e-8bea5faea546",
   "metadata": {},
   "source": [
    "- To determine which model is the better performer, we need to evaluate their performance on a validation set using an appropriate metric such as Mean Squared Error (MSE) or R-squared. However, in general, Lasso regularization tends to perform better when the data has a large number of features with some of them being more important than others, while Ridge regularization tends to perform better when the data has many features with similar importance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
