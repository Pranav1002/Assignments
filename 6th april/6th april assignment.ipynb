{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2801bc9-9e00-4094-bab5-8163e2c8f86a",
   "metadata": {},
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e567dd-9b7d-4cfd-bc3e-fe0969e55d8f",
   "metadata": {},
   "source": [
    "Given a training dataset consisting of input vectors X and corresponding class labels y, where X is an n-dimensional feature vector and y is either -1 or +1 indicating the class labels, the goal of a linear SVM is to find a hyperplane that maximally separates the two classes.\n",
    "\n",
    "The hyperplane is defined by the equation:\n",
    "\n",
    "w^T * x + b = 0,\n",
    "\n",
    "where w is the weight vector perpendicular to the hyperplane, x is the input vector, and b is the bias term. The weight vector w determines the orientation of the hyperplane, and the bias term b shifts the hyperplane along the w direction.\n",
    "\n",
    "The decision function of the SVM is given by:\n",
    "\n",
    "f(x) = sign(w^T * x + b),\n",
    "\n",
    "where sign(x) is the sign function that returns -1 if x < 0, +1 if x > 0, and 0 if x = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29e7ad-dfbf-4c52-b3b1-541f1cdaa80f",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06323bc0-d274-4614-bf19-672f76cb68eb",
   "metadata": {},
   "source": [
    "# Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e58289-fa27-43fb-a7d0-b336bf8e59ed",
   "metadata": {},
   "source": [
    "- The objective of the SVM algorithm is to find a hyperplane that, to the best degree possible, separates data points of one class from those of another class.\n",
    "- The SVM algorithm tries to find the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future.\n",
    "- The objective function of a linear SVM is to maximize the margin between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c84576-e046-4db7-8098-9ef58a8fb249",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45416e-381c-43e6-9291-6f202ea96436",
   "metadata": {},
   "source": [
    "# Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a61a8-78df-41e8-8914-85ef225b86d9",
   "metadata": {},
   "source": [
    "- The kernel trick is a powerful technique that enables SVMs to solve non-linear classification problems by implicitly mapping the input data to a higher-dimensional feature space. By doing so, it allows us to find a hyperplane that separates the different classes of data.\n",
    "- ure space. By doing so, it allows us to find a hyperplane that separates the different classes of data1. The “Kernel Trick” is a method used in Support Vector Machines (SVMs) to convert data (that is not linearly separable) into a higher-dimensional feature space where it may be linearly separated.\n",
    "-  Internally, the kernelized SVM can compute these complex transformations just in terms of similarity calculations between pairs of points in the higher dimensional feature space where the transformed feature representation is implicit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150fbba-f556-4d4d-8021-20a956e883aa",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0bf46-456f-496a-b87b-484a82a6b251",
   "metadata": {},
   "source": [
    "# Q4. What is the role of support vectors in SVM Explain with example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a2494-34d8-46e0-899d-1a5280066f80",
   "metadata": {},
   "source": [
    "- Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier.\n",
    "- The goal of SVM is to find the hyperplane that maximizes the margin between the two classes. The margin is defined as the distance between the separating hyperplane (decision boundary) and the closest points of each class.\n",
    "- The hyperplane with maximum margin is called the optimal hyperplane. The points closest to the hyperplane are called support vectors.\n",
    "- For example, consider a dataset with two classes that are linearly separable. In this case, we can find multiple hyperplanes that can separate these two classes. However, we want to find a hyperplane that maximizes the margin between these two classes. This is where support vectors come into play. Support vectors are data points that lie closest to the decision boundary (hyperplane). These points are used to define the margin and hence influence the position and orientation of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4734500-7318-4ba9-9f3e-6b8fabb86f5a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d0197c-6b31-4fdc-a8f7-2665ffecd047",
   "metadata": {},
   "source": [
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5309fe-ed21-41da-8e14-29e2446398bd",
   "metadata": {},
   "source": [
    "1. Hyperplane:\n",
    "- In SVM, a hyperplane is a decision boundary that separates data points into different classes. For binary classification, a hyperplane is a line in a 2D space, a plane in a 3D space, or a hyperplane in higher dimensions. It aims to maximize the margin between the classes, which are the closest points to the decision boundary.\n",
    "Example:\n",
    "Consider a simple 2D dataset with two classes, represented by red and blue points. The hyperplane is represented by the black line, which effectively separates the two classes.\n",
    "\n",
    "2. Marginal Plane:\n",
    "- The marginal plane refers to the parallel planes that lie on both sides of the hyperplane and touch the support vectors (data points closest to the hyperplane). The distance between the hyperplane and the marginal plane is called the margin.\n",
    "Example:\n",
    "Let's consider the same dataset as before. The marginal planes are depicted by the dashed lines. They touch the support vectors (represented by the filled circles) and define the width of the margin.\n",
    "\n",
    "3. Soft Margin:\n",
    "- In some cases, the data may not be linearly separable, or there may be outliers that make it difficult to find a hyperplane with a wide margin. In such cases, we can introduce a soft margin to allow some misclassification or errors.\n",
    "Example:\n",
    "Suppose we have a dataset with overlapping classes and outliers. By introducing a soft margin, SVM allows for some misclassification, indicated by the circled points. The dashed lines represent the marginal planes.\n",
    "\n",
    "4. Hard Margin:\n",
    "- On the other hand, a hard margin SVM requires that all data points are correctly classified, and there are no outliers. It aims to find a hyperplane that separates the classes with a maximum margin without allowing any misclassification.\n",
    "Example:\n",
    "If we have a linearly separable dataset with no outliers, a hard margin SVM can find a hyperplane (represented by the black line) that perfectly separates the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e561ef-a07f-4bbd-955d-36f4e7c043bd",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87f39d-d980-4d4f-854e-0d324a1e3d40",
   "metadata": {},
   "source": [
    "# Q6. SVM Implementation through Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa0026a-6ed3-48d0-8651-eccab66952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f397e605-b840-4378-9cab-aa71ed420e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba3a917-9782-47a4-9720-df96c2c40c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d31f951-f2d8-42c9-9a7b-9d2fbaeb855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51155b22-a964-42e1-9c4e-3a2c0f59b56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4fd2248-57e4-412a-88b8-ec770f52ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31b9078-cfc6-467d-96ee-052dc0239d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c3bc04-654e-4a9f-a145-dc99ad162a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1 Accuracy: 1.0\n",
      "C = 1 Accuracy: 1.0\n",
      "C = 10 Accuracy: 0.9666666666666667\n",
      "C = 100 Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "C_values = [0.1, 1, 10, 100]\n",
    "for C in C_values:\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(x_train, y_train)\n",
    "    y_pred = svm.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"C =\", C, \"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717f6b7-b804-4f4e-b952-4739f46462a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
